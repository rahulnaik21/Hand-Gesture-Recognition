{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9ef739-b777-4ed0-98e3-0265f24cfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui  \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05034912-a8e1-4598-b36d-9a04c3275976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyAutoGUI for media playback control\n",
    "media_player = pyautogui\n",
    "\n",
    "# Initialize MediaPipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands() \n",
    "\n",
    "# Initialize MediaPipe eye detection model\n",
    "mp_eye_detection = mp.solutions.face_detection\n",
    "eye_detection = mp_eye_detection.FaceDetection()\n",
    "\n",
    "# Function to count fingers   count the number of extended fingers based on the detected hand landmarks. \n",
    "def count_fingers(lst):\n",
    "    cnt = 0  #lst=contain hand landmarks detected by the MediaPipe library.\n",
    "\n",
    "    thresh = (lst.landmark[0].y * 100 - lst.landmark[9].y * 100) / 2  #vertical distance b/w 1st and 9th landmark.  determine whether a finger is extended or not.\n",
    "\n",
    "    if (lst.landmark[5].y * 100 - lst.landmark[8].y * 100) > thresh:\n",
    "        cnt += 1     #it indicates that the index finger is extended, so cnt is incremented by 1.\n",
    "\n",
    "    if (lst.landmark[9].y * 100 - lst.landmark[12].y * 100) > thresh:\n",
    "        cnt += 1  #it indicates that the middle finger is extended, so cnt is incremented by 1.\n",
    "\n",
    "    if (lst.landmark[13].y * 100 - lst.landmark[16].y * 100) > thresh:\n",
    "        cnt += 1  #indicates ring finger\n",
    "\n",
    "    if (lst.landmark[17].y * 100 - lst.landmark[20].y * 100) > thresh:\n",
    "        cnt += 1  #liitle finger\n",
    "    return cnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a65fb27-a053-40a8-9bad-6cd6e7706de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect eyes\n",
    "def detect_eyes(frame):\n",
    "    # Convert the frame to RGB format\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #MediaPipe library processes frames in RGB format.\n",
    "    #using OpenCV's cvtColor function\n",
    "    # Process the RGB frame using the eye detection model\n",
    "    results = eye_detection.process(rgb_frame)\n",
    "    eyes_detected = False\n",
    "    #previously initialized eye_detection model to process the RGB frame and detect eyes\n",
    "\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            left_eye = [pt for pt in detection.location_data.relative_keypoints if 0.4 < pt.x < 0.5]\n",
    "            #Extracts the keypoints corresponding to the left eye by filtering the keypoints based on\n",
    "            # their relative x-coordinates. \n",
    "            #Only keypoints with x-coordinates between 0.4 and 0.5 are considered as part of the left eye.\n",
    "            right_eye = [pt for pt in detection.location_data.relative_keypoints if 0.5 < pt.x < 0.6]\n",
    "            \n",
    "            if left_eye and right_eye:\n",
    "                eyes_detected = True\n",
    "                break\n",
    "    return eyes_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf475ff-ad6f-4704-8565-163533e31bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop for capturing video and detecting eyes and hand gestures\n",
    "cap = cv2.VideoCapture(0)\n",
    "eyes_status = False  # Variable to track eyes status\n",
    "prev = -1\n",
    "start_init = False \n",
    "\n",
    "while True:\n",
    "    end_time = time.time()\n",
    "    _, frm = cap.read()\n",
    "    frm = cv2.flip(frm, 1) # flips the captured frame horizontally (around the y-axis) using OpenCV's flip function. It's often done to correct the orientation of the frame.\n",
    "\n",
    "    # Detect hands in the frame\n",
    "    res = hands.process(cv2.cvtColor(frm, cv2.COLOR_BGR2RGB))#The easiest way to convert is to use openCV cvtColor\n",
    "\n",
    "    # Detect eyes in the frame\n",
    "    eyes_detected = detect_eyes(frm)  #detect eyes in the flipped frame\n",
    "\n",
    "    # If eyes status changes, play/pause media playback\n",
    "    if eyes_detected != eyes_status:  #eye status has changed compared to the previous iteration and updating\n",
    "        eyes_status = eyes_detected\n",
    "        if eyes_detected:                  #control media playback using PyAutoGUI.\n",
    "            media_player.press('space')  # Press spacebar to play\n",
    "        else:\n",
    "            media_player.press ('space')  # Press spacebar to pause\n",
    "\n",
    "    if res.multi_hand_landmarks:\n",
    "        hand_keyPoints = res.multi_hand_landmarks[0]  # checks any hand landmarks detection and retriving its value\n",
    "\n",
    "        cnt = count_fingers(hand_keyPoints)  #count the number of extended fingers based on the detected hand landmarks. \n",
    "\n",
    "        if not(prev==cnt): \n",
    "            if not(start_init)    :   # checks if the gesture recognition process has not yet started. If start_init is False, it means this is the first time a change in finger count is detected.\n",
    "                start_time = time.time()  #start a timer if the finger count changes for gesture recognition.\n",
    "                start_init = True       # gesture recognition process has started.\n",
    "            elif (end_time-start_time) > 0.2:\n",
    "                if (cnt == 1):\n",
    "                    pyautogui.press(\"right\")\n",
    "                elif (cnt == 2):\n",
    "                    pyautogui.press(\"left\")\n",
    "                elif (cnt == 3):\n",
    "                    pyautogui.press(\"up\")\n",
    "                elif (cnt == 4):\n",
    "                    pyautogui.press(\"down\")\n",
    "\n",
    "                prev = cnt\n",
    "                start_init = False    # resets to 0 and indicates that it is completed\n",
    "   # Display the frame\n",
    "    cv2.imshow(\"window\", frm)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        cv2.destroyAllWindows()   #(escape)discard all frames and breaking the loop\n",
    "        cap.release()   #freeing up the camera resource. \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4719d-5540-4e1c-8fd4-eeeab38ac6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7b307-f641-498a-9d70-5d1d3923db28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (mediapipe)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
